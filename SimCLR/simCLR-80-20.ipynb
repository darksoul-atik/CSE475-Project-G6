{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12397312,"sourceType":"datasetVersion","datasetId":7817780}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Part 0: Install dependencies** ","metadata":{}},{"cell_type":"code","source":"# === Part 0: Install dependencies (Lightly) ===\n!pip -q install lightly\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 1 — Setup & Config","metadata":{}},{"cell_type":"code","source":"# === Part 1: Setup & Config ===\nimport os, math, random, numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ---- Lightly (for SimCLR pretraining option) ----\nfrom lightly.loss import NTXentLoss\nfrom lightly.models.modules import SimCLRProjectionHead\nfrom lightly.transforms.simclr_transform import SimCLRTransform\n\n# -----------------------\n# User toggles\n# -----------------------\nENHANCED = False            # set True to try safe boosts (recommended to try later)\nUSE_LIGHTLY_SIMCLR = True   # <- set True to use Lightly SimCLR pretraining you provided\nDATA_DIR = '/kaggle/input/riceds-original/Original'\nSAVE_DIR = './'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nBATCH_SIZE = 64\nNUM_WORKERS = 4\nSEED = 42\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# SimCLR pretrain (shared)\nSIMCLR_EPOCHS = 100          # try 200–400 if you can afford it\nSIMCLR_LR = 3e-4\nSIMCLR_WEIGHT_DECAY = 1e-6\nTEMPERATURE = 0.2\n\n# Supervised fine-tune (NO FREEZING)\nFINETUNE_EPOCHS = 30\nFT_LR_BACKBONE = 1e-4        # lower LR for encoder\nFT_LR_HEAD = 1e-3            # higher LR for classifier head\nFT_WEIGHT_DECAY = 1e-4\nLABEL_SMOOTH = 0.0           # if using MixUp, keep 0 or very small\n\nUSE_IMAGENET_WEIGHTS = True\nUSE_EMA = True               # EMA of fine-tune weights\nEMA_DECAY = 0.999\nUSE_MIXUP = True\nMIXUP_ALPHA = 0.2\nUSE_TTA = True               # eval-time flip TTA\n\n# Enhancements (only used if ENHANCED=True)\nWARMUP_RATIO = 0.1           # 10% warmup\nUSE_BALANCED_SAMPLER = True  # class-balanced sampler for FT\nTRACK_BN_IN_EMA = True       # include BN running stats in EMA\nRUN_KNN_EVAL = True          # quick SSL sanity check\n\n# Histories for curves\nsimclr_loss_hist, ft_loss_hist, ft_acc_hist, val_acc_hist = [], [], [], []\n\n# Reproducibility\ntorch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\nimport torch.backends.cudnn as cudnn\ncudnn.deterministic = True; cudnn.benchmark = False\n\nprint(\"Device:\", DEVICE)\nprint(\"Enhanced mode:\", ENHANCED)\nprint(\"Using Lightly for SimCLR pretraining:\", USE_LIGHTLY_SIMCLR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2 — Augmentations","metadata":{}},{"cell_type":"code","source":"# === Part 2: Augmentations ===\n# SimCLR two-crop transform (native)\nclass TwoCropsTransform:\n    \"\"\"Return two random augmentations of the same image (SimCLR).\"\"\"\n    def __init__(self, size=224, enhanced=False):\n        normalize = transforms.Normalize([0.485, 0.456, 0.406],\n                                         [0.229, 0.224, 0.225])\n        if enhanced:\n            # scale blur kernel to image size\n            k = int(size * 0.07) // 2 * 2 + 1  # ~7% of size, odd\n        else:\n            k = 9\n        self.transform = transforms.Compose([\n            transforms.RandomResizedCrop(size, scale=(0.08, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.GaussianBlur(kernel_size=k, sigma=(0.1, 2.0)),\n            transforms.ToTensor(),\n            normalize,\n        ])\n    def __call__(self, x):\n        return self.transform(x), self.transform(x)\n\n# Supervised transforms\nsupervised_train_tf = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225]),\n])\neval_tf = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225]),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3 — Stratified Split (80/20)","metadata":{}},{"cell_type":"code","source":"# === Part 3: Stratified 80/20 split BEFORE training ===\n_from_split = datasets.ImageFolder(DATA_DIR, transform=transforms.ToTensor())\nlabels_all = [lbl for _, lbl in _from_split.samples]\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\ntrain_idx, test_idx = next(sss.split(np.zeros(len(labels_all)), labels_all))\n\nlen_train, len_test = len(train_idx), len(test_idx)\nprint(f\"Train size: {len_train}, Test size: {len_test}, Classes: {len(_from_split.classes)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 4 — SSL Datasets & Loaders","metadata":{}},{"cell_type":"code","source":"# === Part 4: SSL datasets/loaders with SAME indices ===\nif USE_LIGHTLY_SIMCLR:\n    # Lightly's SimCLRTransform returns two crops as a tuple\n    ssl_dataset = datasets.ImageFolder(\n        DATA_DIR,\n        transform=SimCLRTransform(input_size=224, gaussian_blur=1.0)  # blur on for 224\n    )\nelse:\n    ssl_dataset = datasets.ImageFolder(\n        DATA_DIR,\n        transform=TwoCropsTransform(enhanced=ENHANCED)\n    )\n\ntrain_ssl = Subset(ssl_dataset, train_idx)\n\ntrain_loader_ssl = DataLoader(\n    train_ssl,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=True,                 # BN stability\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 5 — Models (Encoder, Projector, Sup Head) + Losses","metadata":{}},{"cell_type":"code","source":"# === Part 5: Models & Losses ===\nclass Encoder(nn.Module):\n    def __init__(self, use_imagenet=True):\n        super().__init__()\n        if use_imagenet:\n            try:\n                base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n            except Exception:\n                base = models.resnet50(pretrained=True)\n        else:\n            base = models.resnet50(weights=None)\n        self.backbone = nn.Sequential(*list(base.children())[:-1])  # (B, 2048, 1, 1)\n        self.feature_dim = 2048\n    def forward(self, x):\n        x = self.backbone(x)\n        return torch.flatten(x, 1)                                   # (B, 2048)\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, bn_last=False):\n        super().__init__()\n        layers = [\n            nn.Linear(in_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        ]\n        if bn_last:\n            layers.append(nn.BatchNorm1d(out_dim, affine=False))\n        self.net = nn.Sequential(*layers)\n    def forward(self, x):\n        return self.net(x)\n\n# Native SimCLR (your original)\nclass SimCLR_Native(nn.Module):\n    def __init__(self, encoder, proj_dim=256, hidden=2048):\n        super().__init__()\n        self.encoder = encoder\n        self.projector = MLP(encoder.feature_dim, hidden, proj_dim, bn_last=True)\n    def forward(self, x1, x2):\n        h1 = self.encoder(x1); h2 = self.encoder(x2)\n        z1 = self.projector(h1); z2 = self.projector(h2)\n        return z1, z2\n\n# Lightly-style SimCLR (uses Lightly's projection head and loss)\nclass SimCLR_Lightly(nn.Module):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder                  # same resnet50 backbone encoder\n        self.projection_head = SimCLRProjectionHead(encoder.feature_dim, 512, 128)\n    def forward_single(self, x):\n        h = self.encoder(x)                     # (B, 2048)\n        z = self.projection_head(h)\n        return z\n    def forward(self, x1, x2):\n        return self.forward_single(x1), self.forward_single(x2)\n\n# InfoNCE (native). Kept for native branch\ndef nt_xent_loss_native(z1, z2, temperature=0.2):\n    b = z1.size(0)\n    z1 = F.normalize(z1, dim=1).float()\n    z2 = F.normalize(z2, dim=1).float()\n    z  = torch.cat([z1, z2], dim=0)                 # (2B, d)\n    logits = z @ z.t() / float(temperature)         # (2B, 2B)\n    eye = torch.eye(2*b, device=z.device, dtype=torch.bool)\n    logits.masked_fill_(eye, -1e9)                  # remove self\n    targets = (torch.arange(2*b, device=z.device) + b) % (2*b)\n    return F.cross_entropy(logits, targets)\n\n# Supervised model\nclass SupModel(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        self.encoder = encoder\n        self.head = nn.Linear(encoder.feature_dim, num_classes)\n    def forward(self, x):\n        feats = self.encoder(x)                      # (B, 2048)\n        return self.head(feats)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 6 — SimCLR Pretraining (Native or Lightly)","metadata":{}},{"cell_type":"code","source":"# === Part 6: SimCLR Pretraining (Native OR Lightly) ===\n# This cell saves encoder weights to enc_path; downstream parts remain unchanged.\nenc_path = os.path.join(SAVE_DIR, \"simclr_encoder.pth\")\n\nif USE_LIGHTLY_SIMCLR:\n    # ----- Lightly branch (implements your provided example at 224px / ResNet50) -----\n    encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n    simclr = SimCLR_Lightly(encoder).to(DEVICE)\n\n    # Lightly's NTXentLoss\n    criterion = NTXentLoss(temperature=TEMPERATURE)\n\n    # Optimizer (use AdamW like your setup; feel free to try SGD lr=0.06 if you want to mirror the example)\n    optimizer = torch.optim.AdamW(simclr.parameters(), lr=SIMCLR_LR, weight_decay=SIMCLR_WEIGHT_DECAY)\n\n    # Optional warmup for enhanced mode\n    def cosine_with_warmup(optim, warmup_steps, total_steps):\n        def lr_lambda(step):\n            if step < warmup_steps: return step / max(1, warmup_steps)\n            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n            return 0.5 * (1 + math.cos(math.pi * progress))\n        return torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda)\n\n    if ENHANCED:\n        total_steps = SIMCLR_EPOCHS * len(train_loader_ssl)\n        warmup_steps = int(WARMUP_RATIO * total_steps)\n        sched = cosine_with_warmup(optimizer, warmup_steps, total_steps)\n    else:\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SIMCLR_EPOCHS * len(train_loader_ssl))\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n\n    print(\"Starting Lightly SimCLR pretraining\")\n    for epoch in range(SIMCLR_EPOCHS):\n        simclr.train()\n        total_loss = 0.0\n        pbar = tqdm(train_loader_ssl, desc=f\"Lightly SimCLR {epoch+1}/{SIMCLR_EPOCHS}\")\n        for i, batch in enumerate(pbar, start=1):\n            # Lightly's SimCLRTransform returns a tuple (x0, x1) as batch[0]\n            if isinstance(batch[0], (tuple, list)) and len(batch[0]) == 2:\n                x0, x1 = batch[0]\n            else:\n                # fallback if transform behaved differently\n                (x0, x1), _ = batch\n            x0 = x0.to(DEVICE, non_blocking=True)\n            x1 = x1.to(DEVICE, non_blocking=True)\n\n            with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n                z0 = simclr.forward_single(x0)\n                z1 = simclr.forward_single(x1)\n                loss = criterion(z0, z1)\n\n            optimizer.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += float(loss.detach().item())\n            if ENHANCED:\n                sched.step()\n            else:\n                # step-per-iteration cosine to mimic flat+cos behavior\n                sched.step()\n\n            pbar.set_postfix(loss=f\"{total_loss/i:.4f}\")\n\n        epoch_loss = total_loss / len(train_loader_ssl)\n        simclr_loss_hist.append(epoch_loss)\n        print(f\"Lightly SimCLR Epoch {epoch+1}: loss={epoch_loss:.4f}\")\n\n    # Save encoder after Lightly SimCLR pretrain\n    torch.save(simclr.encoder.state_dict(), enc_path)\n    print(f\"Saved SimCLR encoder to: {enc_path}\")\n\nelse:\n    # ----- Native branch (your original SimCLR) -----\n    encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n    simclr = SimCLR_Native(encoder).to(DEVICE)\n\n    ssl_optimizer = torch.optim.AdamW(simclr.parameters(), lr=SIMCLR_LR, weight_decay=SIMCLR_WEIGHT_DECAY)\n    ssl_sched = torch.optim.lr_scheduler.CosineAnnealingLR(ssl_optimizer, T_max=SIMCLR_EPOCHS)\n\n    # Optional warmup\n    def cosine_with_warmup(optimizer, warmup_steps, total_steps):\n        def lr_lambda(step):\n            if step < warmup_steps:\n                return step / max(1, warmup_steps)\n            progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n            return 0.5 * (1 + math.cos(math.pi * progress))\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    if ENHANCED:\n        total_steps = SIMCLR_EPOCHS * len(train_loader_ssl)\n        warmup_steps = int(WARMUP_RATIO * total_steps)\n        ssl_sched = cosine_with_warmup(ssl_optimizer, warmup_steps, total_steps)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n\n    for epoch in range(SIMCLR_EPOCHS):\n        simclr.train()\n        running = 0.0\n        pbar = tqdm(train_loader_ssl, desc=f\"SimCLR Epoch {epoch+1}/{SIMCLR_EPOCHS}\")\n        for i, ((v1, v2), _) in enumerate(pbar, start=1):\n            x1 = v1.to(DEVICE, non_blocking=True)\n            x2 = v2.to(DEVICE, non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n                z1, z2 = simclr(x1, x2)\n                loss = nt_xent_loss_native(z1, z2, temperature=TEMPERATURE)\n            ssl_optimizer.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.step(ssl_optimizer)\n            scaler.update()\n            running += loss.item()\n            pbar.set_postfix(loss=f\"{running / i:.4f}\")\n\n        epoch_loss = running / len(train_loader_ssl)\n        simclr_loss_hist.append(epoch_loss)\n        ssl_sched.step()\n        print(f\"SimCLR Epoch {epoch+1}: loss={epoch_loss:.4f}\")\n\n    # Save encoder after native SimCLR pretrain\n    torch.save(simclr.encoder.state_dict(), enc_path)\n    print(f\"Saved SimCLR encoder to: {enc_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 7 — Supervised Fine-Tuning (NO Freezing) + EMA + MixUp + TTA","metadata":{}},{"cell_type":"code","source":"# === Part 7: Supervised Fine-Tuning (EMA + MixUp + TTA) ===\nsup_train_ds = datasets.ImageFolder(DATA_DIR, transform=supervised_train_tf)\nsup_test_ds  = datasets.ImageFolder(DATA_DIR, transform=eval_tf)\nnum_classes = len(sup_train_ds.classes)\n\ntrain_sup = Subset(sup_train_ds, train_idx)\ntest_sup  = Subset(sup_test_ds,  test_idx)\n\n# Optional class-balanced sampler (enhanced)\nif ENHANCED and USE_BALANCED_SAMPLER:\n    targets = np.array([sup_train_ds.samples[i][1] for i in train_idx])\n    class_counts = np.bincount(targets, minlength=num_classes).astype(float)\n    class_weights = 1.0 / np.maximum(class_counts, 1.0)\n    sample_weights = class_weights[targets]\n    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n    train_loader_sup = DataLoader(train_sup, batch_size=BATCH_SIZE, sampler=sampler,\n                                  num_workers=NUM_WORKERS, pin_memory=True)\nelse:\n    train_loader_sup = DataLoader(train_sup, batch_size=BATCH_SIZE, shuffle=True,\n                                  num_workers=NUM_WORKERS, pin_memory=True)\n\ntest_loader_sup = DataLoader(test_sup, batch_size=BATCH_SIZE, shuffle=False,\n                             num_workers=NUM_WORKERS, pin_memory=True)\n\nclass EMA:\n    def __init__(self, model, decay=0.999, track_bn=False):\n        self.decay = decay\n        self.shadow = {}\n        for k, v in model.state_dict().items():\n            if v.dtype.is_floating_point:\n                if track_bn or ('running' not in k):\n                    self.shadow[k] = v.detach().clone()\n    @torch.no_grad()\n    def update(self, model):\n        for k, v in model.state_dict().items():\n            if k in self.shadow and v.dtype.is_floating_point:\n                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1 - self.decay)\n    @torch.no_grad()\n    def copy_to(self, model):\n        sd = model.state_dict()\n        for k, v in self.shadow.items():\n            sd[k].copy_(v)\n\ndef one_hot_with_smoothing(y, num_classes, eps=0.0):\n    y = y.view(-1)\n    oh = torch.zeros(y.size(0), num_classes, device=y.device)\n    oh.scatter_(1, y.unsqueeze(1), 1.0)\n    if eps > 0:\n        oh = oh * (1 - eps) + eps / num_classes\n    return oh\n\ndef mixup_data(x, y, alpha=0.2, num_classes=1000, eps=0.0):\n    if alpha <= 0:\n        return x, one_hot_with_smoothing(y, num_classes, eps), 1.0\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0), device=x.device)\n    x_mix = lam * x + (1 - lam) * x[index]\n    y1 = one_hot_with_smoothing(y, num_classes, eps)\n    y2 = y1[index]\n    y_mix = lam * y1 + (1 - lam) * y2\n    return x_mix, y_mix, lam\n\ndef soft_cross_entropy(logits, target_prob):\n    log_prob = F.log_softmax(logits, dim=1)\n    return -(target_prob * log_prob).sum(dim=1).mean()\n\ndef accuracy_top1(logits, targets):\n    return (logits.argmax(dim=1) == targets).float().mean().item()\n\ndef predict_tta(model, x):\n    logits1 = model(x)\n    logits2 = model(torch.flip(x, dims=[3]))  # horizontal flip\n    return (logits1 + logits2) / 2\n\nfinetune_encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\nfinetune_encoder.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"simclr_encoder.pth\"), map_location=DEVICE))\nsup_model = SupModel(finetune_encoder, num_classes).to(DEVICE)\n\nparam_groups = [\n    {\"params\": sup_model.encoder.parameters(), \"lr\": FT_LR_BACKBONE, \"weight_decay\": FT_WEIGHT_DECAY},\n    {\"params\": sup_model.head.parameters(),    \"lr\": FT_LR_HEAD,     \"weight_decay\": FT_WEIGHT_DECAY},\n]\nft_optimizer = torch.optim.AdamW(param_groups)\n\n# Scheduler\ndef cosine_with_warmup(optimizer, warmup_steps, total_steps):\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return step / max(1, warmup_steps)\n        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n        return 0.5 * (1 + math.cos(math.pi * progress))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nif ENHANCED:\n    total_steps = FINETUNE_EPOCHS * len(train_loader_sup)\n    warmup_steps = int(WARMUP_RATIO * total_steps)\n    ft_sched = cosine_with_warmup(ft_optimizer, warmup_steps, total_steps)\nelse:\n    ft_sched = torch.optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max=FINETUNE_EPOCHS)\n\nema = EMA(sup_model, decay=EMA_DECAY, track_bn=(ENHANCED and TRACK_BN_IN_EMA)) if USE_EMA else None\nscaler_ft = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n\nbest_acc = 0.0\nbest_path = os.path.join(SAVE_DIR, \"simclr_finetune_best.pt\")\n\nfor epoch in range(FINETUNE_EPOCHS):\n    sup_model.train()\n    run_loss, run_acc = 0.0, 0.0\n    pbar = tqdm(train_loader_sup, desc=f\"FT Epoch {epoch+1}/{FINETUNE_EPOCHS}\")\n    for i, (x, y) in enumerate(pbar, start=1):\n        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n        if USE_MIXUP:\n            x_in, y_soft, _ = mixup_data(x, y, alpha=MIXUP_ALPHA, num_classes=num_classes, eps=LABEL_SMOOTH)\n        else:\n            x_in, y_soft = x, one_hot_with_smoothing(y, num_classes, eps=LABEL_SMOOTH)\n\n        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n            logits = sup_model(x_in)\n            loss = soft_cross_entropy(logits, y_soft)\n\n        ft_optimizer.zero_grad(set_to_none=True)\n        scaler_ft.scale(loss).backward()\n\n        if ENHANCED:\n            scaler_ft.unscale_(ft_optimizer)\n            torch.nn.utils.clip_grad_norm_(sup_model.parameters(), 5.0)\n            scaler_ft.step(ft_optimizer)\n        else:\n            torch.nn.utils.clip_grad_norm_(sup_model.parameters(), 5.0)\n            scaler_ft.step(ft_optimizer)\n\n        scaler_ft.update()\n        if USE_EMA:\n            ema.update(sup_model)\n\n        with torch.no_grad():\n            logits_nomix = sup_model(x)\n            run_acc += accuracy_top1(logits_nomix, y)\n\n        run_loss += loss.item()\n        pbar.set_postfix(loss=f\"{run_loss/i:.4f}\", acc=f\"{(run_acc/i):.4f}\")\n\n        if ENHANCED:\n            ft_sched.step()\n\n    epoch_train_loss = run_loss / len(train_loader_sup)\n    epoch_train_acc  = run_acc  / len(train_loader_sup)\n    ft_loss_hist.append(epoch_train_loss)\n    ft_acc_hist.append(epoch_train_acc)\n    if not ENHANCED:\n        ft_sched.step()\n    print(f\"[FT] Epoch {epoch+1}: loss={epoch_train_loss:.4f} | acc={epoch_train_acc:.4f}\")\n\n    # --- Eval each epoch with EMA + TTA ---\n    sup_model.eval()\n    backup = {k: v.detach().clone() for k, v in sup_model.state_dict().items() if v.dtype.is_floating_point}\n    if USE_EMA:\n        ema.copy_to(sup_model)\n\n    correct, total = 0, 0\n    with torch.no_grad():\n        for x, y in test_loader_sup:\n            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n            logits = predict_tta(sup_model, x) if USE_TTA else sup_model(x)\n            pred = logits.argmax(dim=1)\n            correct += (pred == y).sum().item()\n            total += y.numel()\n    epoch_acc = correct / max(total, 1)\n    val_acc_hist.append(epoch_acc)\n\n    sd = sup_model.state_dict()\n    for k, v in backup.items():\n        sd[k].copy_(v)\n\n    if epoch_acc > best_acc:\n        best_acc = epoch_acc\n        if USE_EMA:\n            ema.copy_to(sup_model)\n            torch.save(sup_model.state_dict(), best_path)\n            for k, v in backup.items():\n                sd[k].copy_(v)\n        else:\n            torch.save(sup_model.state_dict(), best_path)\n\n    print(f\"[Eval] top1={epoch_acc:.4f} (best={best_acc:.4f})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 8 — (Optional) k-NN Sanity Check on SSL Features","metadata":{}},{"cell_type":"code","source":"# === Part 8 (Optional): k-NN eval on features ===\nif ENHANCED and RUN_KNN_EVAL:\n    @torch.no_grad()\n    def extract_feats(model, loader, device):\n        model.eval()\n        feats, labels = [], []\n        for x, y in loader:\n            x = x.to(device, non_blocking=True)\n            f = model.encoder(x)\n            f = F.normalize(f, dim=1)\n            feats.append(f.cpu()); labels.append(y)\n        return torch.cat(feats), torch.cat(labels)\n\n    from sklearn.neighbors import KNeighborsClassifier\n    train_feats, train_y = extract_feats(sup_model, train_loader_sup, DEVICE)\n    val_feats,   val_y   = extract_feats(sup_model, test_loader_sup,  DEVICE)\n    knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n    knn.fit(train_feats.numpy(), train_y.numpy())\n    print(\"kNN top-1:\", knn.score(val_feats.numpy(), val_y.numpy()))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 9 — Final Evaluation (Held-out 20%) + Report","metadata":{}},{"cell_type":"code","source":"# === Part 9: Final evaluation on held-out 20% (EMA best) ===\nfinetune_encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\nenc_path = os.path.join(SAVE_DIR, \"simclr_encoder.pth\")\nbest_path = os.path.join(SAVE_DIR, \"simclr_finetune_best.pt\")\n\nfinetune_encoder.load_state_dict(torch.load(enc_path, map_location=DEVICE))\nsup_model = SupModel(finetune_encoder, num_classes).to(DEVICE)\nsup_model.load_state_dict(torch.load(best_path, map_location=DEVICE))\nsup_model.eval()\n\ny_true, y_pred = [], []\nwith torch.no_grad():\n    for x, y in test_loader_sup:\n        x = x.to(DEVICE, non_blocking=True)\n        logits = (predict_tta(sup_model, x) if USE_TTA else sup_model(x))\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(y.numpy())\n\nprint(\"\\n=== SimCLR -> End-to-End Fine-Tune: Held-out Test ===\")\nprint(classification_report(y_true, y_pred, digits=4))\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\nplt.title(\"Confusion Matrix (SimCLR Fine-Tune, EMA+MixUp+TTA)\")\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\nplt.tight_layout(); plt.savefig(os.path.join(SAVE_DIR, \"simclr_confusion_matrix.png\"), dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 10 — Training Curves","metadata":{}},{"cell_type":"code","source":"# === Part 10: Training Curves ===\nplt.figure(figsize=(7,4))\nplt.plot(simclr_loss_hist, marker='o')\nplt.title(\"SimCLR Pretraining Loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, \"simclr_pretrain_loss.png\"), dpi=150)\nplt.show()\n\nfig, ax = plt.subplots(1, 3, figsize=(18,4))\nax[0].plot(ft_loss_hist, marker='o')\nax[0].set_title(\"Fine-tune Loss\"); ax[0].set_xlabel(\"Epoch\"); ax[0].set_ylabel(\"Loss\"); ax[0].grid(True, alpha=0.3)\n\nax[1].plot(ft_acc_hist, marker='o', label='Train')\nax[1].plot(val_acc_hist, marker='s', label='Val (EMA+TTA)')\nax[1].set_title(\"Accuracy\"); ax[1].set_xlabel(\"Epoch\"); ax[1].set_ylabel(\"Top-1 Acc\"); ax[1].grid(True, alpha=0.3); ax[1].legend()\n\nax[2].plot(simclr_loss_hist, marker='o')\nax[2].set_title(\"Pretrain Loss\"); ax[2].set_xlabel(\"Epoch\"); ax[2].set_ylabel(\"Loss\"); ax[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nfig.savefig(os.path.join(SAVE_DIR, \"simclr_training_curves.png\"), dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 11 — Collect predictions and probabilities (for CM & ROC/AUC)","metadata":{}},{"cell_type":"code","source":"# === Part 11: Gather predictions, probabilities, and class names ===\nimport numpy as np\nimport torch.nn.functional as F\n\nsup_model.eval()\nclass_names = sup_test_ds.classes\nnum_classes = len(class_names)\n\nall_probs, all_preds, all_targets = [], [], []\n\nwith torch.no_grad():\n    for x, y in test_loader_sup:\n        x = x.to(DEVICE, non_blocking=True)\n        logits = (predict_tta(sup_model, x) if USE_TTA else sup_model(x))\n        probs  = F.softmax(logits, dim=1).cpu().numpy()\n        preds  = np.argmax(probs, axis=1)\n        all_probs.append(probs)\n        all_preds.append(preds)\n        all_targets.append(y.numpy())\n\ny_prob = np.concatenate(all_probs, axis=0)      # (N, C)\ny_pred = np.concatenate(all_preds, axis=0)      # (N,)\ny_true = np.concatenate(all_targets, axis=0)    # (N,)\n\nprint(\"Shapes -> y_prob:\", y_prob.shape, \"| y_pred:\", y_pred.shape, \"| y_true:\", y_true.shape)\nprint(\"Classes:\", class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 12 — Confusion Matrix (counts & normalized)","metadata":{}},{"cell_type":"code","source":"# === Part 12: Confusion Matrix (Counts & Normalized) ===\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ncm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\ncm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-9)\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n            xticklabels=class_names, yticklabels=class_names)\naxes[0].set_title(\"Confusion Matrix (Counts)\")\naxes[0].set_xlabel(\"Predicted\"); axes[0].set_ylabel(\"True\")\n\nsns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens', ax=axes[1],\n            xticklabels=class_names, yticklabels=class_names, vmin=0, vmax=1)\naxes[1].set_title(\"Confusion Matrix (Normalized)\")\naxes[1].set_xlabel(\"Predicted\"); axes[1].set_ylabel(\"True\")\n\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, \"confusion_matrix_counts_normalized.png\"), dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 13 — ROC Curves & AUC (one-vs-rest, micro & macro)","metadata":{}},{"cell_type":"code","source":"# === Part 13: ROC & AUC (One-vs-Rest, Micro, Macro) ===\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\n\ny_true_bin = label_binarize(y_true, classes=list(range(num_classes)))  # (N, C)\n\nfpr = dict(); tpr = dict(); roc_auc = dict()\nfor c in range(num_classes):\n    fpr[c], tpr[c], _ = roc_curve(y_true_bin[:, c], y_prob[:, c])\n    roc_auc[c] = auc(fpr[c], tpr[c])\n\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\nall_fpr = np.unique(np.concatenate([fpr[c] for c in range(num_classes)]))\nmean_tpr = np.zeros_like(all_fpr)\nfor c in range(num_classes):\n    mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\nmean_tpr /= num_classes\nroc_auc[\"macro\"] = auc(all_fpr, mean_tpr)\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], linestyle='-', label=f\"micro-average ROC (AUC = {roc_auc['micro']:.4f})\")\nplt.plot(all_fpr,   mean_tpr,        linestyle='--', label=f\"macro-average ROC (AUC = {roc_auc['macro']:.4f})\")\n\nmax_shown = 10 if num_classes > 10 else num_classes\nfor c in range(max_shown):\n    plt.plot(fpr[c], tpr[c], label=f\"{class_names[c]} (AUC = {roc_auc[c]:.3f})\", alpha=0.8)\n\nplt.plot([0, 1], [0, 1], linestyle=':')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"One-vs-Rest ROC Curves\")\nplt.legend(loc=\"lower right\", fontsize=9)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, \"roc_ovr_micro_macro.png\"), dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 14 — Clustering of Features (K-Means, ARI/NMI, t-SNE viz)","metadata":{}},{"cell_type":"code","source":"# === Part 14: Clustering on encoder features ===\nimport torch\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\nfrom sklearn.manifold import TSNE\n\n@torch.no_grad()\ndef extract_features_only(model, loader, device):\n    model.eval()\n    feats, labels = [], []\n    for x, y in loader:\n        x = x.to(device, non_blocking=True)\n        f = model.encoder(x)         # (B, 2048)\n        feats.append(f.cpu().numpy()); labels.append(y.numpy())\n    return np.concatenate(feats, 0), np.concatenate(labels, 0)\n\nfeatures, labels = extract_features_only(sup_model, test_loader_sup, DEVICE)\nfeatures_norm = features / (np.linalg.norm(features, axis=1, keepdims=True) + 1e-9)\n\nkmeans = KMeans(n_clusters=num_classes, n_init=20, random_state=SEED)\nclusters = kmeans.fit_predict(features_norm)\n\nari = adjusted_rand_score(labels, clusters)\nnmi = normalized_mutual_info_score(labels, clusters)\nsil = silhouette_score(features_norm, clusters)\nprint(f\"K-Means on encoder features -> ARI: {ari:.4f} | NMI: {nmi:.4f} | Silhouette: {sil:.4f}\")\n\nprint(\"Running t-SNE (this can take ~30–60s on CPU for large N)...\")\ntsne = TSNE(n_components=2, perplexity=min(30, max(5, len(labels)//20)), init='pca', random_state=SEED)\nemb2d = tsne.fit_transform(features_norm)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\nscatter1 = axes[0].scatter(emb2d[:,0], emb2d[:,1], c=labels, s=12, alpha=0.9, cmap='tab20')\naxes[0].set_title(\"t-SNE colored by TRUE class\")\naxes[0].axis('off')\nlegend1 = axes[0].legend(*scatter1.legend_elements(num=num_classes), title=\"Class\", bbox_to_anchor=(1.02, 1), loc='upper left')\naxes[0].add_artist(legend1)\n\nscatter2 = axes[1].scatter(emb2d[:,0], emb2d[:,1], c=clusters, s=12, alpha=0.9, cmap='tab20')\naxes[1].set_title(\"t-SNE colored by K-Means CLUSTER\")\naxes[1].axis('off')\nlegend2 = axes[1].legend(*scatter2.legend_elements(num=num_classes), title=\"Cluster\", bbox_to_anchor=(1.02, 1), loc='upper left')\n\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, \"tsne_true_vs_cluster.png\"), dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 15 — Single-Image Validation Demo (path or random test sample)","metadata":{}},{"cell_type":"code","source":"# === Part 15: Single Image Validation Demo ===\nfrom PIL import Image\nimport torch\nimport numpy as np\n\nimg_path = None  # e.g., \"/kaggle/input/riceds-original/Original/classX/img123.jpg\"\n\nif img_path is None:\n    import random\n    rnd_idx = random.choice(range(len(test_sup)))\n    pil_img_transformed, true_label = test_sup[rnd_idx]\n    raw_path, raw_lbl = sup_test_ds.samples[test_idx[rnd_idx]]\n    rgb_img = Image.open(raw_path).convert(\"RGB\")\n    tensor_img = eval_tf(rgb_img).unsqueeze(0).to(DEVICE)\nelse:\n    rgb_img = Image.open(img_path).convert(\"RGB\")\n    tensor_img = eval_tf(rgb_img).unsqueeze(0).to(DEVICE)\n    true_label = None  # unknown unless from test set\n\nsup_model.eval()\nwith torch.no_grad():\n    logits = (predict_tta(sup_model, tensor_img) if USE_TTA else sup_model(tensor_img))\n    probs = F.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n\ntopk = min(5, num_classes)\ntop_idx = np.argsort(-probs)[:topk]\ntop_pairs = [(class_names[i], float(probs[i])) for i in top_idx]\n\nplt.figure(figsize=(5,5))\nplt.imshow(rgb_img); plt.axis('off')\ntitle = \"Prediction\"\nif true_label is not None:\n    title += f\" | True: {class_names[true_label]}\"\nplt.title(title)\nplt.show()\n\nprint(\"Top-k probabilities:\")\nfor name, p in top_pairs:\n    print(f\"{name:>20s}: {p:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 16A — Grad-CAM per-class summary (recommended)","metadata":{}},{"cell_type":"code","source":"# === Part 16A: Grad-CAM per-class summary (correct & incorrect) ===\nimport os, numpy as np, torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\n\nOUT_DIR = os.path.join(SAVE_DIR, \"gradcam_per_class\")\nos.makedirs(OUT_DIR, exist_ok=True)\nN_PER_CLASS_CORRECT = 1\nN_PER_CLASS_INCORRECT = 1\n\nclass GradCAM:\n    def __init__(self, model: nn.Module, target_module: nn.Module):\n        self.model = model\n        self.target_module = target_module\n        self.activations = None\n        self.gradients = None\n        self.fwd = target_module.register_forward_hook(self._fwd_hook)\n        self.bwd = target_module.register_full_backward_hook(self._bwd_hook)\n    def _fwd_hook(self, m, i, o): self.activations = o.detach()\n    def _bwd_hook(self, m, gi, go): self.gradients = go[0].detach()\n    def remove(self): self.fwd.remove(); self.bwd.remove()\n    def generate(self, target_idx):\n        w = self.gradients.mean(dim=(2,3), keepdim=True)\n        cam = (w * self.activations).sum(dim=1, keepdim=True)\n        cam = F.relu(cam)\n        B = cam.shape[0]\n        cam = cam.view(B, -1); cam -= cam.min(dim=1, keepdim=True).values\n        cam /= (cam.max(dim=1, keepdim=True).values + 1e-9)\n        return cam.view(-1, 1, self.activations.shape[2], self.activations.shape[3])\n\ndef get_target_conv_module(sup_model: nn.Module, name='layer4'):\n    return sup_model.encoder.backbone[7]  # ResNet50 layer4\n\ndef overlay_and_save(rgb_img, cam_upsampled, title, fpath):\n    plt.figure(figsize=(8,4))\n    plt.subplot(1,2,1); plt.imshow(rgb_img); plt.axis('off'); plt.title(title)\n    plt.subplot(1,2,2); plt.imshow(rgb_img); plt.imshow(cam_upsampled, cmap='jet', alpha=0.35); plt.axis('off'); plt.title(\"Grad-CAM\")\n    plt.tight_layout(); plt.savefig(fpath, dpi=150); plt.close()\n\n# Pass 1: collect candidate indices per class\ncands_correct = {c: [] for c in range(len(class_names))}\ncands_incorrect = {c: [] for c in range(len(class_names))}\n\nsup_model.eval()\ntest_samples = [sup_test_ds.samples[i] for i in test_idx]  # (path, label)\n\nfor idx_in_subset, (x_t, y_t) in enumerate(tqdm(test_loader_sup, desc=\"Scanning test set\")):\n    for j in range(x_t.size(0)):\n        global_j = idx_in_subset * test_loader_sup.batch_size + j\n        if global_j >= len(test_samples): break\n        raw_path, true_lbl = test_samples[global_j]\n        x = x_t[j:j+1].to(DEVICE, non_blocking=True)\n        probs = F.softmax(sup_model(x), dim=1)[0].detach().cpu().numpy()\n        pred = int(np.argmax(probs))\n        conf = float(probs[pred])\n        entry = (conf, raw_path, true_lbl, pred)\n        if pred == true_lbl:\n            cands_correct[true_lbl].append(entry)\n        else:\n            cands_incorrect[true_lbl].append(entry)\n\nfor c in range(len(class_names)):\n    cands_correct[c].sort(key=lambda t: -t[0])\n    cands_incorrect[c].sort(key=lambda t: -t[0])\n\n# Pass 2: Grad-CAM overlays\ntarget_module = get_target_conv_module(sup_model, 'layer4')\nfor c in range(len(class_names)):\n    # Corrects\n    for k, entry in enumerate(cands_correct[c][:N_PER_CLASS_CORRECT]):\n        conf, raw_path, true_lbl, pred = entry\n        rgb = Image.open(raw_path).convert(\"RGB\")\n        x = eval_tf(rgb).unsqueeze(0).to(DEVICE)\n        cam_engine = GradCAM(sup_model, target_module)\n        with torch.enable_grad():\n            x.requires_grad_(True)\n            logits = sup_model(x)\n            score = logits[0, pred]\n            sup_model.zero_grad(set_to_none=True)\n            score.backward()\n            cam = cam_engine.generate(pred)[0,0]\n            cam_engine.remove()\n        cam_up = F.interpolate(cam.unsqueeze(0).unsqueeze(0), size=x.shape[-2:], mode='bilinear', align_corners=False)[0,0].cpu().numpy()\n        title = f\"TRUE={class_names[true_lbl]} | PRED={class_names[pred]} ({conf:.3f}) [correct]\"\n        fpath = os.path.join(OUT_DIR, f\"class{c:02d}_correct_{k}.png\")\n        overlay_and_save(rgb, cam_up, title, fpath)\n\n    # Incorrects\n    for k, entry in enumerate(cands_incorrect[c][:N_PER_CLASS_INCORRECT]):\n        conf, raw_path, true_lbl, pred = entry\n        rgb = Image.open(raw_path).convert(\"RGB\")\n        x = eval_tf(rgb).unsqueeze(0).to(DEVICE)\n        cam_engine = GradCAM(sup_model, target_module)\n        with torch.enable_grad():\n            x.requires_grad_(True)\n            logits = sup_model(x)\n            score = logits[0, pred]\n            sup_model.zero_grad(set_to_none=True)\n            score.backward()\n            cam = cam_engine.generate(pred)[0,0]\n            cam_engine.remove()\n        cam_up = F.interpolate(cam.unsqueeze(0).unsqueeze(0), size=x.shape[-2:], mode='bilinear', align_corners=False)[0,0].cpu().numpy()\n        title = f\"TRUE={class_names[true_lbl]} | PRED={class_names[pred]} ({conf:.3f}) [incorrect]\"\n        fpath = os.path.join(OUT_DIR, f\"class{c:02d}_incorrect_{k}.png\")\n        overlay_and_save(rgb, cam_up, title, fpath)\n\nprint(f\"Saved per-class Grad-CAM overlays to: {OUT_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}